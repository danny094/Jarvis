# core/layers/control.py
"""
LAYER 2: ControlLayer (Qwen)
v4.0: Echtes Ollama Streaming für Progressive Steps
"""

import json
import httpx
import re
import asyncio
from typing import Dict, Any, Optional, AsyncGenerator
from config import OLLAMA_BASE, CONTROL_MODEL, THINKING_MODEL
from utils.logger import log_info, log_error, log_debug, log_warning
from utils.json_parser import safe_parse_json
from core.safety import LightCIM
from core.sequential_registry import get_registry

CIM_URL = "http://cim-server:8086"

CONTROL_PROMPT = """Du bist der CONTROL-Layer eines AI-Systems.
Deine Aufgabe: Überprüfe den Plan vom Thinking-Layer BEVOR eine Antwort generiert wird.

Du antwortest NUR mit validem JSON, nichts anderes.

JSON-Format:
{
    "approved": true/false,
    "corrections": {
        "needs_memory": null oder true/false,
        "memory_keys": null oder ["korrigierte", "keys"],
        "hallucination_risk": null oder "low/medium/high",
        "new_fact_key": null oder "korrigierter_key",
        "new_fact_value": null oder "korrigierter_value"
    },
    "warnings": ["Liste von Warnungen falls vorhanden"],
    "final_instruction": "Klare Anweisung für den Output-Layer"
}
"""

SEQUENTIAL_SYSTEM_PROMPT = """You are a rigorous step-by-step reasoner.

Format your response with clear step markers:
## Step 1: [Step Title]
[Your detailed analysis for this step]

## Step 2: [Step Title]
[Your detailed analysis for this step]

IMPORTANT:
- Start each step with "## Step N:" on its own line
- Give each step a descriptive title
- Be thorough but concise
- Complete all requested steps"""


class ControlLayer:
    def __init__(self, model: str = CONTROL_MODEL):
        self.model = model
        self.ollama_base = OLLAMA_BASE
        self.sequential_model = THINKING_MODEL
        self.light_cim = LightCIM()
        self.mcp_hub = None
        self.registry = get_registry()
    
    def set_mcp_hub(self, hub):
        self.mcp_hub = hub
        log_info("[ControlLayer] MCP Hub connected")
    
    async def verify(self, user_text: str, thinking_plan: Dict[str, Any], retrieved_memory: str = "") -> Dict[str, Any]:
        sequential_result = thinking_plan.get("_sequential_result")
        if not sequential_result:
            sequential_result = await self._check_sequential_thinking(user_text, thinking_plan)
        if sequential_result:
            log_info(f"[ControlLayer] Sequential completed with {len(sequential_result.get('steps', []))} steps")
            thinking_plan["_sequential_result"] = sequential_result
        
        try:
            cim_result = self.light_cim.validate_basic(
                intent=thinking_plan.get("intent", ""),
                hallucination_risk=thinking_plan.get("hallucination_risk", "low"),
                user_text=user_text,
                thinking_plan=thinking_plan
            )
            log_info(f"[LightCIM] safe={cim_result['safe']}, confidence={cim_result['confidence']:.2f}")
            if not cim_result["safe"]:
                return {"approved": False, "corrections": {}, "warnings": cim_result["warnings"],
                        "final_instruction": "Request blocked", "_light_cim": cim_result}
        except Exception as e:
            log_error(f"[LightCIM] Error: {e}")
        
        prompt = f"""{CONTROL_PROMPT}

USER-ANFRAGE: {user_text}
PLAN: {json.dumps(thinking_plan, indent=2, ensure_ascii=False)}
MEMORY: {retrieved_memory if retrieved_memory else "(keine)"}

Deine Bewertung (nur JSON):"""

        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                r = await client.post(f"{self.ollama_base}/api/generate",
                    json={"model": self.model, "prompt": prompt, "stream": False, "format": "json"})
                r.raise_for_status()
            data = r.json()
            content = data.get("response", "").strip() or data.get("thinking", "").strip()
            if not content:
                return self._default_verification(thinking_plan)
            return safe_parse_json(content, default=self._default_verification(thinking_plan), context="ControlLayer")
        except Exception as e:
            log_error(f"[ControlLayer] Error: {e}")
            return self._default_verification(thinking_plan)
    
    async def _check_sequential_thinking(self, user_text: str, thinking_plan: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        if not thinking_plan.get("needs_sequential_thinking", False):
            return None
        if not self.mcp_hub:
            log_error("[ControlLayer] MCP Hub not connected!")
            return None
        complexity = thinking_plan.get("sequential_complexity", 5)
        log_info(f"[ControlLayer] Triggering Sequential (complexity={complexity})")
        try:
            result = self.mcp_hub.call_tool("think", {"message": user_text, "steps": complexity})
            if isinstance(result, dict) and "error" in result:
                log_error(f"[ControlLayer] Sequential failed: {result['error']}")
                return None
            task_id = self.registry.create_task(user_text, complexity)
            self.registry.update_status(task_id, "running")
            self.registry.set_result(task_id, result)
            return result
        except Exception as e:
            log_error(f"[ControlLayer] Sequential failed: {e}")
            return None

    async def _get_cim_context(self, user_text: str, mode: str = None) -> Optional[str]:
        try:
            async with httpx.AsyncClient(timeout=10.0) as client:
                init = await client.post(f"{CIM_URL}/mcp", json={
                    "jsonrpc": "2.0", "id": 0, "method": "initialize",
                    "params": {"protocolVersion": "2024-11-05", "capabilities": {},
                               "clientInfo": {"name": "control-layer", "version": "4.0.0"}}
                }, headers={"Content-Type": "application/json"})
                session_id = init.headers.get("mcp-session-id")
                if not session_id:
                    return None
                resp = await client.post(f"{CIM_URL}/mcp", json={
                    "jsonrpc": "2.0", "id": 1, "method": "tools/call",
                    "params": {"name": "analyze", "arguments": {"query": user_text, "mode": mode}}
                }, headers={"Content-Type": "application/json", "mcp-session-id": session_id})
                if resp.status_code == 200:
                    for line in resp.text.split("\n"):
                        if line.startswith("data: "):
                            data = json.loads(line[6:])
                            if "result" in data:
                                content = data["result"].get("content", [])
                                if content:
                                    result = json.loads(content[0].get("text", "{}"))
                                    return result.get("causal_prompt", "")
        except Exception as e:
            log_debug(f"[CIM] Not available: {e}")
        return None

    async def _check_sequential_thinking_stream(self, user_text: str, thinking_plan: Dict[str, Any]) -> AsyncGenerator[Dict[str, Any], None]:
        """v4.0: ECHTES Ollama Streaming - Steps werden LIVE erkannt!"""
        import uuid
        
        if not thinking_plan.get("needs_sequential_thinking", False):
            return
        
        task_id = f"seq-{str(uuid.uuid4())[:8]}"
        complexity = thinking_plan.get("sequential_complexity", 5)
        cim_modes = thinking_plan.get("suggested_cim_modes", [])
        reasoning_type = thinking_plan.get("reasoning_type", "direct")
        
        log_info(f"[ControlLayer] Sequential STREAM (complexity={complexity}, id={task_id})")
        
        yield {"type": "sequential_start", "task_id": task_id, "complexity": complexity,
               "cim_modes": cim_modes, "reasoning_type": reasoning_type}
        
        try:
            cim_context = None
            if cim_modes:
                cim_context = await self._get_cim_context(user_text, cim_modes[0] if cim_modes else None)
            
            system_prompt = SEQUENTIAL_SYSTEM_PROMPT
            if cim_context:
                system_prompt += f"\n\n=== CIM CONTEXT ===\n{cim_context}\n=== END ==="
            
            user_prompt = f"Analyze using exactly {complexity} steps:\n\nQUERY: {user_text}\n\nStart each step with '## Step N: [Title]'."
            
            log_info(f"[ControlLayer] Starting Ollama stream...")
            
            payload = {"model": self.sequential_model, "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ], "stream": True}
            
            accumulated = ""
            current_step_num = 0
            current_step_title = ""
            current_step_content = ""
            all_steps = []
            step_pattern = re.compile(r'^## Step (\d+):\s*(.+)$', re.MULTILINE)
            
            async with httpx.AsyncClient(timeout=300.0) as client:
                async with client.stream("POST", f"{self.ollama_base}/api/chat", json=payload) as response:
                    async for line in response.aiter_lines():
                        if not line.strip():
                            continue
                        try:
                            chunk = json.loads(line)
                        except json.JSONDecodeError:
                            continue
                        
                        content = chunk.get("message", {}).get("content", "")
                        if not content:
                            continue
                        
                        accumulated += content
                        
                        for match in step_pattern.finditer(accumulated):
                            step_num = int(match.group(1))
                            step_title = match.group(2).strip()
                            
                            if step_num <= current_step_num:
                                continue
                            
                            if current_step_num > 0 and current_step_content.strip():
                                step_data = {"step": current_step_num, "step_id": f"step_{current_step_num}",
                                            "title": current_step_title, "thought": current_step_content.strip(), "status": "complete"}
                                all_steps.append(step_data)
                                log_info(f"[ControlLayer] YIELD Step {current_step_num}: {current_step_title[:30]}...")
                                yield {"type": "sequential_step", "task_id": task_id, "step_number": current_step_num,
                                       "total_steps": complexity, "step": current_step_title,
                                       "thought": current_step_content.strip(), "status": "complete"}
                            
                            current_step_num = step_num
                            current_step_title = step_title
                            marker_end = match.end()
                            remaining = accumulated[marker_end:]
                            next_match = step_pattern.search(remaining)
                            current_step_content = remaining[:next_match.start()] if next_match else remaining
                        
                        if current_step_num > 0:
                            marker = f"## Step {current_step_num}:"
                            pos = accumulated.rfind(marker)
                            if pos >= 0:
                                after = accumulated[pos:]
                                nl = after.find('\n')
                                if nl >= 0:
                                    txt = after[nl + 1:]
                                    nm = step_pattern.search(txt)
                                    current_step_content = txt[:nm.start()] if nm else txt
                        
                        if chunk.get("done", False):
                            break
            
            if current_step_num > 0 and current_step_content.strip():
                step_data = {"step": current_step_num, "step_id": f"step_{current_step_num}",
                            "title": current_step_title, "thought": current_step_content.strip(), "status": "complete"}
                all_steps.append(step_data)
                log_info(f"[ControlLayer] YIELD Final Step {current_step_num}")
                yield {"type": "sequential_step", "task_id": task_id, "step_number": current_step_num,
                       "total_steps": complexity, "step": current_step_title,
                       "thought": current_step_content.strip(), "status": "complete"}
            
            result = {"success": True, "steps": all_steps, "input": user_text,
                     "cim_enabled": cim_context is not None, "full_response": accumulated}
            
            registry_id = self.registry.create_task(user_text, complexity)
            self.registry.update_status(registry_id, "completed")
            self.registry.set_result(registry_id, result)
            thinking_plan["_sequential_result"] = result
            
            log_info(f"[ControlLayer] Sequential {task_id} STREAMED with {len(all_steps)} steps")
            yield {"type": "sequential_done", "task_id": task_id, "steps": all_steps,
                   "summary": f"{len(all_steps)} steps completed (streamed)"}
            
        except Exception as e:
            log_error(f"[ControlLayer] Stream failed: {e}")
            yield {"type": "sequential_error", "task_id": task_id, "error": str(e)}

    def _default_verification(self, thinking_plan: Dict[str, Any]) -> Dict[str, Any]:
        return {"approved": True, "corrections": {"needs_memory": None, "memory_keys": None,
                "hallucination_risk": None, "new_fact_key": None, "new_fact_value": None},
                "warnings": ["Control-Layer Fallback"], "final_instruction": "Beantworte vorsichtig."}
    
    def apply_corrections(self, thinking_plan: Dict[str, Any], verification: Dict[str, Any]) -> Dict[str, Any]:
        corrected = thinking_plan.copy()
        for k, v in verification.get("corrections", {}).items():
            if v is not None:
                corrected[k] = v
        corrected["_verified"] = True
        corrected["_final_instruction"] = verification.get("final_instruction", "")
        corrected["_warnings"] = verification.get("warnings", [])
        return corrected
