"""
Sequential Thinking MCP Server v2.1

Now integrated with CIM (Causal Intelligence Module) for:
- Pre-step validation (anti-patterns, biases)
- Post-step validation (fallacies, logic gates)
- Course correction when needed

Port: 8085
Calls: CIM Server on port 8086

v2.1 Changes:
- Fixed CIM connection: Now uses proper Session Management
- Added Accept headers for FastMCP Streamable HTTP
- SSE response parsing
"""

import os
import json
import httpx
import re
from typing import Optional, Dict, Any, List
from fastmcp import FastMCP

# Configuration
CIM_URL = os.environ.get("CIM_URL", "http://cim-server:8086")
OLLAMA_BASE = os.environ.get("OLLAMA_BASE", "http://ollama:11434")
OLLAMA_MODEL = os.environ.get("OLLAMA_MODEL", "deepseek-r1:8b")

# Initialize MCP Server
mcp = FastMCP("sequential_thinking")


# ============================================================
# CIM CLIENT - Calls to CIM MCP Server (FIXED v2.1)
# ============================================================

class CIMClient:
    """
    HTTP client for CIM MCP Server.
    
    v2.1: Proper FastMCP Streamable HTTP support with Session Management.
    """
    
    def __init__(self, base_url: str):
        self.base_url = base_url
        self.timeout = 30.0
        self._session_id: Optional[str] = None
        self._initialized = False
    
    def _get_headers(self) -> Dict[str, str]:
        """Get headers required for FastMCP Streamable HTTP."""
        headers = {
            "Content-Type": "application/json",
            "Accept": "application/json, text/event-stream"
        }
        if self._session_id:
            headers["mcp-session-id"] = self._session_id
        return headers
    
    def _parse_sse_response(self, text: str) -> Dict[str, Any]:
        """
        Parse SSE (Server-Sent Events) response from FastMCP.
        
        Format:
          event: message
          data: {"jsonrpc":"2.0","id":1,"result":{...}}
        """
        # Find the data line
        for line in text.split("\n"):
            if line.startswith("data: "):
                json_str = line[6:]  # Remove "data: " prefix
                try:
                    return json.loads(json_str)
                except json.JSONDecodeError:
                    pass
        
        # Fallback: try parsing whole text as JSON
        try:
            return json.loads(text)
        except json.JSONDecodeError:
            return {"error": f"Could not parse response: {text[:200]}"}
    
    async def _ensure_session(self, client: httpx.AsyncClient) -> bool:
        """
        Initialize MCP session if not already done.
        
        Returns True if session is ready, False on error.
        """
        if self._initialized and self._session_id:
            return True
        
        try:
            init_payload = {
                "jsonrpc": "2.0",
                "id": 0,
                "method": "initialize",
                "params": {
                    "protocolVersion": "2024-11-05",
                    "capabilities": {},
                    "clientInfo": {
                        "name": "sequential-thinking",
                        "version": "2.1.0"
                    }
                }
            }
            
            response = await client.post(
                f"{self.base_url}/mcp",
                json=init_payload,
                headers=self._get_headers()
            )
            
            # Get session ID from response headers
            self._session_id = response.headers.get("mcp-session-id")
            
            if response.status_code == 200 and self._session_id:
                self._initialized = True
                print(f"[CIMClient] Session initialized: {self._session_id[:8]}...")
                return True
            else:
                print(f"[CIMClient] Session init failed: {response.status_code}")
                return False
                
        except Exception as e:
            print(f"[CIMClient] Session init error: {e}")
            return False
    
    async def call_tool(self, tool_name: str, arguments: Dict[str, Any]) -> Dict[str, Any]:
        """
        Call a CIM tool via MCP protocol.
        
        v2.1: Now with proper Session Management and SSE parsing.
        """
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                # Ensure we have a valid session
                if not await self._ensure_session(client):
                    return {"error": "Failed to establish CIM session"}
                
                # Make the tool call
                response = await client.post(
                    f"{self.base_url}/mcp",
                    json={
                        "jsonrpc": "2.0",
                        "id": 1,
                        "method": "tools/call",
                        "params": {
                            "name": tool_name,
                            "arguments": arguments
                        }
                    },
                    headers=self._get_headers()
                )
                
                if response.status_code != 200:
                    return {"error": f"CIM returned {response.status_code}: {response.text[:200]}"}
                
                # Parse SSE response
                result = self._parse_sse_response(response.text)
                
                # Extract result from JSON-RPC response
                if "result" in result:
                    content = result["result"]
                    # Handle MCP content format
                    if isinstance(content, dict) and "content" in content:
                        items = content["content"]
                        if isinstance(items, list) and len(items) > 0:
                            text = items[0].get("text", "{}")
                            try:
                                return json.loads(text) if isinstance(text, str) else text
                            except json.JSONDecodeError:
                                return {"raw_text": text}
                    return content
                elif "error" in result:
                    return {"error": result["error"]}
                
                return result
                
        except httpx.ConnectError:
            self._initialized = False
            self._session_id = None
            return {"error": f"Cannot connect to CIM server at {self.base_url}"}
        except Exception as e:
            return {"error": str(e)}
    
    async def analyze(self, query: str, mode: Optional[str] = None) -> Dict[str, Any]:
        """Build causal graph for query."""
        return await self.call_tool("analyze", {
            "query": query,
            "mode": mode,
            "include_visual": False,
            "include_prompt": True
        })
    
    async def validate_before(self, step_description: str, step_id: str, context: str = None) -> Dict[str, Any]:
        """Validate step before execution."""
        return await self.call_tool("validate_before", {
            "step_description": step_description,
            "step_id": step_id,
            "context": context
        })
    
    async def validate_after(self, step_id: str, step_result: str, expected: str = None) -> Dict[str, Any]:
        """Validate step after execution."""
        return await self.call_tool("validate_after", {
            "step_id": step_id,
            "step_result": step_result,
            "expected_outcome": expected
        })
    
    async def correct_course(self, step_id: str, current_plan: str, violations: List[str] = None) -> Dict[str, Any]:
        """Get corrected plan."""
        return await self.call_tool("correct_course", {
            "step_id": step_id,
            "current_plan": current_plan,
            "violations": violations
        })
    
    async def health_check(self) -> Dict[str, Any]:
        """Check CIM server health."""
        return await self.call_tool("health", {})


# Initialize CIM Client
cim = CIMClient(CIM_URL)


# ============================================================
# OLLAMA CLIENT - For LLM Reasoning
# ============================================================

async def call_ollama(prompt: str, system: str = None) -> str:
    """Call Ollama for LLM reasoning."""
    try:
        messages = []
        if system:
            messages.append({"role": "system", "content": system})
        messages.append({"role": "user", "content": prompt})
        
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                f"{OLLAMA_BASE}/api/chat",
                json={
                    "model": OLLAMA_MODEL,
                    "messages": messages,
                    "stream": False
                }
            )
            response.raise_for_status()
            result = response.json()
            return result.get("message", {}).get("content", "")
            
    except Exception as e:
        return f"[Ollama Error: {e}]"


# ============================================================
# MCP TOOLS
# ============================================================

@mcp.tool()
async def think(
    message: str,
    steps: int = 3,
    mode: Optional[str] = None,
    use_cim: bool = True
) -> Dict[str, Any]:
    """
    Sequential thinking with CIM validation.
    
    For each step:
    1. BEFORE: CIM validates the plan (anti-patterns, biases)
    2. EXECUTE: Ollama generates reasoning
    3. AFTER: CIM validates the result (fallacies, logic)
    
    Args:
        message: The query/task to think through
        steps: Number of reasoning steps (default: 3)
        mode: Force CIM mode (light, heavy, strategic, temporal, simulation)
        use_cim: Enable CIM validation (default: True)
    
    Returns:
        Chain of reasoning steps with validation results
    """
    chain = []
    context_accumulator = ""
    corrections_made = 0
    cim_errors = []
    
    # Step 0: Get initial causal graph from CIM
    initial_analysis = None
    causal_prompt = None
    
    if use_cim:
        print(f"[Sequential] Calling CIM.analyze for: {message[:50]}...")
        initial_analysis = await cim.analyze(message, mode)
        
        if initial_analysis.get("error"):
            cim_errors.append(f"analyze: {initial_analysis['error']}")
            print(f"[Sequential] CIM analyze error: {initial_analysis['error']}")
        elif initial_analysis.get("success"):
            causal_prompt = initial_analysis.get("causal_prompt", "")
            print(f"[Sequential] CIM mode: {initial_analysis.get('mode_selected')}")
    
    # Execute each step
    for i in range(steps):
        step_id = f"step_{i + 1}"
        step_data = {
            "step": i + 1,
            "step_id": step_id,
            "status": "pending"
        }
        
        # Determine what this step should do
        if i == 0:
            step_task = f"Analyze and understand the problem: {message}"
        elif i == steps - 1:
            step_task = f"Synthesize findings and provide final answer for: {message}"
        else:
            step_task = f"Continue reasoning about: {message}\nContext so far: {context_accumulator[:500]}"
        
        # ============================================================
        # 1. BEFORE: CIM Pre-validation
        # ============================================================
        if use_cim:
            before_result = await cim.validate_before(
                step_description=step_task,
                step_id=step_id,
                context=context_accumulator
            )
            
            if before_result.get("error"):
                cim_errors.append(f"validate_before[{step_id}]: {before_result['error']}")
                step_data["cim_before"] = {"error": before_result["error"]}
            else:
                step_data["cim_before"] = {
                    "safe": before_result.get("safe", True),
                    "violations": before_result.get("violations", []),
                    "warnings": before_result.get("warnings", [])
                }
                
                # If derailed, get correction
                if before_result.get("derailed"):
                    correction = await cim.correct_course(
                        step_id=step_id,
                        current_plan=step_task,
                        violations=before_result.get("warnings", [])
                    )
                    
                    if correction.get("success"):
                        step_task = correction.get("corrected_prompt", step_task)
                        corrections_made += 1
                        step_data["correction_applied"] = True
        
        # ============================================================
        # 2. EXECUTE: Ollama LLM Call
        # ============================================================
        system_prompt = "You are a careful, step-by-step reasoner. Think through problems methodically."
        if causal_prompt:
            system_prompt += f"\n\nCausal Context:\n{causal_prompt}"
        
        llm_prompt = f"""Step {i + 1} of {steps}: {step_task}

Previous context:
{context_accumulator if context_accumulator else "(This is the first step)"}

Provide your reasoning for this step. Be thorough but concise."""

        thought = await call_ollama(llm_prompt, system_prompt)
        step_data["thought"] = thought
        step_data["task"] = step_task
        
        # ============================================================
        # 3. AFTER: CIM Post-validation
        # ============================================================
        if use_cim:
            after_result = await cim.validate_after(
                step_id=step_id,
                step_result=thought,
                expected=None
            )
            
            if after_result.get("error"):
                cim_errors.append(f"validate_after[{step_id}]: {after_result['error']}")
                step_data["cim_after"] = {"error": after_result["error"]}
            else:
                step_data["cim_after"] = {
                    "valid": after_result.get("valid", True),
                    "needs_correction": after_result.get("needs_correction", False),
                    "violations": after_result.get("violations", [])
                }
                
                if after_result.get("needs_correction"):
                    step_data["post_validation_warning"] = "Result may contain fallacies"
        
        # Update status and context
        step_data["status"] = "complete"
        context_accumulator += f"\n\n[Step {i + 1}]: {thought}"
        
        chain.append(step_data)
    
    # Build final response
    return {
        "success": True,
        "input": message,
        "steps": chain,
        "total_steps": len(chain),
        "corrections_made": corrections_made,
        "cim_enabled": use_cim,
        "cim_mode": initial_analysis.get("mode_selected") if initial_analysis else None,
        "cim_errors": cim_errors if cim_errors else None,
        "summary": f"{steps} steps completed with {'CIM validation' if use_cim else 'no validation'}"
    }


@mcp.tool()
async def think_simple(message: str, steps: int = 3) -> Dict[str, Any]:
    """
    Simple sequential thinking WITHOUT CIM (for comparison/fallback).
    
    Args:
        message: The query to think through
        steps: Number of steps
    
    Returns:
        Basic reasoning chain
    """
    return await think(message=message, steps=steps, use_cim=False)


@mcp.tool()
async def health() -> Dict[str, Any]:
    """Health check for Sequential Thinking server."""
    # Check CIM connection
    cim_status = "unknown"
    cim_session = None
    try:
        cim_health = await cim.health_check()
        if cim_health.get("error"):
            cim_status = f"error: {cim_health['error']}"
        elif cim_health.get("status") == "healthy":
            cim_status = "connected"
            cim_session = cim._session_id[:8] + "..." if cim._session_id else None
        else:
            cim_status = "unknown"
    except Exception as e:
        cim_status = f"error: {e}"
    
    return {
        "status": "healthy",
        "service": "sequential-thinking",
        "version": "2.1.0",
        "cim_url": CIM_URL,
        "cim_status": cim_status,
        "cim_session": cim_session,
        "ollama_url": OLLAMA_BASE,
        "ollama_model": OLLAMA_MODEL
    }


# ============================================================
# MAIN
# ============================================================

if __name__ == "__main__":
    print("ðŸ§  Starting Sequential Thinking MCP Server v2.1 on port 8085...")
    print(f"   CIM_URL: {CIM_URL}")
    print(f"   OLLAMA_BASE: {OLLAMA_BASE}")
    print("   Tools available:")
    print("   - think: Sequential reasoning with CIM validation")
    print("   - think_simple: Basic reasoning without CIM")
    print("   - health: Health check")
    print("   v2.1: Fixed CIM session management")
    
    mcp.run(
        transport="streamable-http",
        host="0.0.0.0",
        port=8085
    )
