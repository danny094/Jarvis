"""
Sequential Thinking MCP Server v2.0

Now integrated with CIM (Causal Intelligence Module) for:
- Pre-step validation (anti-patterns, biases)
- Post-step validation (fallacies, logic gates)
- Course correction when needed

Port: 8085
Calls: CIM Server on port 8086
"""

import os
import json
import httpx
from typing import Optional, Dict, Any, List
from fastmcp import FastMCP

# Configuration
CIM_URL = os.environ.get("CIM_URL", "http://cim-server:8086")
OLLAMA_BASE = os.environ.get("OLLAMA_BASE", "http://ollama:11434")
OLLAMA_MODEL = os.environ.get("OLLAMA_MODEL", "llama3.1:8b")

# Initialize MCP Server
mcp = FastMCP("sequential_thinking")


# ============================================================
# CIM CLIENT - Calls to CIM MCP Server
# ============================================================

class CIMClient:
    """HTTP client for CIM MCP Server."""
    
    def __init__(self, base_url: str):
        self.base_url = base_url
        self.timeout = 30.0
    
    async def call_tool(self, tool_name: str, arguments: Dict[str, Any]) -> Dict[str, Any]:
        """Call a CIM tool via MCP protocol."""
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                # FastMCP uses streamable-http, we call the tool endpoint
                response = await client.post(
                    f"{self.base_url}/mcp",
                    json={
                        "jsonrpc": "2.0",
                        "id": 1,
                        "method": "tools/call",
                        "params": {
                            "name": tool_name,
                            "arguments": arguments
                        }
                    }
                )
                response.raise_for_status()
                result = response.json()
                
                # Extract result from JSON-RPC response
                if "result" in result:
                    # Parse the content if it's a string
                    content = result["result"]
                    if isinstance(content, dict) and "content" in content:
                        text = content["content"][0].get("text", "{}")
                        return json.loads(text) if isinstance(text, str) else text
                    return content
                elif "error" in result:
                    return {"error": result["error"]}
                return result
                
        except httpx.ConnectError:
            return {"error": f"Cannot connect to CIM server at {self.base_url}"}
        except Exception as e:
            return {"error": str(e)}
    
    async def analyze(self, query: str, mode: Optional[str] = None) -> Dict[str, Any]:
        """Build causal graph for query."""
        return await self.call_tool("analyze", {
            "query": query,
            "mode": mode,
            "include_visual": False,
            "include_prompt": True
        })
    
    async def validate_before(self, step_description: str, step_id: str, context: str = None) -> Dict[str, Any]:
        """Validate step before execution."""
        return await self.call_tool("validate_before", {
            "step_description": step_description,
            "step_id": step_id,
            "context": context
        })
    
    async def validate_after(self, step_id: str, step_result: str, expected: str = None) -> Dict[str, Any]:
        """Validate step after execution."""
        return await self.call_tool("validate_after", {
            "step_id": step_id,
            "step_result": step_result,
            "expected_outcome": expected
        })
    
    async def correct_course(self, step_id: str, current_plan: str, violations: List[str] = None) -> Dict[str, Any]:
        """Get corrected plan."""
        return await self.call_tool("correct_course", {
            "step_id": step_id,
            "current_plan": current_plan,
            "violations": violations
        })


# Initialize CIM Client
cim = CIMClient(CIM_URL)


# ============================================================
# OLLAMA CLIENT - For LLM Reasoning
# ============================================================

async def call_ollama(prompt: str, system: str = None) -> str:
    """Call Ollama for LLM reasoning."""
    try:
        messages = []
        if system:
            messages.append({"role": "system", "content": system})
        messages.append({"role": "user", "content": prompt})
        
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                f"{OLLAMA_BASE}/api/chat",
                json={
                    "model": OLLAMA_MODEL,
                    "messages": messages,
                    "stream": False
                }
            )
            response.raise_for_status()
            result = response.json()
            return result.get("message", {}).get("content", "")
            
    except Exception as e:
        return f"[Ollama Error: {e}]"


# ============================================================
# MCP TOOLS
# ============================================================

@mcp.tool()
async def think(
    message: str,
    steps: int = 3,
    mode: Optional[str] = None,
    use_cim: bool = True
) -> Dict[str, Any]:
    """
    Sequential thinking with CIM validation.
    
    For each step:
    1. BEFORE: CIM validates the plan (anti-patterns, biases)
    2. EXECUTE: Ollama generates reasoning
    3. AFTER: CIM validates the result (fallacies, logic)
    
    Args:
        message: The query/task to think through
        steps: Number of reasoning steps (default: 3)
        mode: Force CIM mode (light, heavy, strategic, temporal, simulation)
        use_cim: Enable CIM validation (default: True)
    
    Returns:
        Chain of reasoning steps with validation results
    """
    chain = []
    context_accumulator = ""
    corrections_made = 0
    
    # Step 0: Get initial causal graph from CIM
    initial_analysis = None
    causal_prompt = None
    
    if use_cim:
        initial_analysis = await cim.analyze(message, mode)
        if initial_analysis.get("success"):
            causal_prompt = initial_analysis.get("causal_prompt", "")
    
    # Execute each step
    for i in range(steps):
        step_id = f"step_{i + 1}"
        step_data = {
            "step": i + 1,
            "step_id": step_id,
            "status": "pending"
        }
        
        # Determine what this step should do
        if i == 0:
            step_task = f"Analyze and understand the problem: {message}"
        elif i == steps - 1:
            step_task = f"Synthesize findings and provide final answer for: {message}"
        else:
            step_task = f"Continue reasoning about: {message}\nContext so far: {context_accumulator[:500]}"
        
        # ============================================================
        # 1. BEFORE: CIM Pre-validation
        # ============================================================
        if use_cim:
            before_result = await cim.validate_before(
                step_description=step_task,
                step_id=step_id,
                context=context_accumulator
            )
            
            step_data["cim_before"] = {
                "safe": before_result.get("safe", True),
                "violations": before_result.get("violations", []),
                "warnings": before_result.get("warnings", [])
            }
            
            # If derailed, get correction
            if before_result.get("derailed"):
                correction = await cim.correct_course(
                    step_id=step_id,
                    current_plan=step_task,
                    violations=before_result.get("warnings", [])
                )
                
                if correction.get("success"):
                    step_task = correction.get("corrected_prompt", step_task)
                    corrections_made += 1
                    step_data["correction_applied"] = True
        
        # ============================================================
        # 2. EXECUTE: Ollama LLM Call
        # ============================================================
        system_prompt = "You are a careful, step-by-step reasoner. Think through problems methodically."
        if causal_prompt:
            system_prompt += f"\n\nCausal Context:\n{causal_prompt}"
        
        llm_prompt = f"""Step {i + 1} of {steps}: {step_task}

Previous context:
{context_accumulator if context_accumulator else "(This is the first step)"}

Provide your reasoning for this step. Be thorough but concise."""

        thought = await call_ollama(llm_prompt, system_prompt)
        step_data["thought"] = thought
        step_data["task"] = step_task
        
        # ============================================================
        # 3. AFTER: CIM Post-validation
        # ============================================================
        if use_cim:
            after_result = await cim.validate_after(
                step_id=step_id,
                step_result=thought,
                expected=None  # Could be enhanced with expected outcomes
            )
            
            step_data["cim_after"] = {
                "valid": after_result.get("valid", True),
                "needs_correction": after_result.get("needs_correction", False),
                "violations": after_result.get("violations", [])
            }
            
            # If needs correction, note it but continue
            # (could be enhanced to retry the step)
            if after_result.get("needs_correction"):
                step_data["post_validation_warning"] = "Result may contain fallacies"
        
        # Update status and context
        step_data["status"] = "complete"
        context_accumulator += f"\n\n[Step {i + 1}]: {thought}"
        
        chain.append(step_data)
    
    # Build final response
    return {
        "success": True,
        "input": message,
        "steps": chain,
        "total_steps": len(chain),
        "corrections_made": corrections_made,
        "cim_enabled": use_cim,
        "cim_mode": initial_analysis.get("mode_selected") if initial_analysis else None,
        "summary": f"{steps} steps completed with {'CIM validation' if use_cim else 'no validation'}"
    }


@mcp.tool()
async def think_simple(message: str, steps: int = 3) -> Dict[str, Any]:
    """
    Simple sequential thinking WITHOUT CIM (for comparison/fallback).
    
    Args:
        message: The query to think through
        steps: Number of steps
    
    Returns:
        Basic reasoning chain
    """
    return await think(message=message, steps=steps, use_cim=False)


@mcp.tool()
async def health() -> Dict[str, Any]:
    """Health check for Sequential Thinking server."""
    # Check CIM connection
    cim_status = "unknown"
    try:
        cim_health = await cim.call_tool("health", {})
        cim_status = "connected" if cim_health.get("status") == "healthy" else "error"
    except:
        cim_status = "disconnected"
    
    return {
        "status": "healthy",
        "service": "sequential-thinking",
        "version": "2.0.0",
        "cim_url": CIM_URL,
        "cim_status": cim_status,
        "ollama_url": OLLAMA_BASE,
        "ollama_model": OLLAMA_MODEL
    }


# ============================================================
# MAIN
# ============================================================

if __name__ == "__main__":
    print("ðŸ§  Starting Sequential Thinking MCP Server v2.0 on port 8085...")
    print(f"   CIM_URL: {CIM_URL}")
    print(f"   OLLAMA_BASE: {OLLAMA_BASE}")
    print("   Tools available:")
    print("   - think: Sequential reasoning with CIM validation")
    print("   - think_simple: Basic reasoning without CIM")
    print("   - health: Health check")
    
    mcp.run(
        transport="streamable-http",
        host="0.0.0.0",
        port=8085
    )
