services:
  lobechat-adapter:
    build:
      context: .
      dockerfile: adapters/lobechat/Dockerfile
    container_name: lobechat-adapter
    ports:
    - 8100:8100
    environment:
    - OLLAMA_BASE=http://ollama:11434
    - MCP_BASE=http://mcp-sql-memory:8081/mcp
    - VALIDATOR_URL=http://validator-service:8000
    - THINKING_MODEL=deepseek-r1:8b
    - CONTROL_MODEL=qwen3:4b
    - EMBEDDING_MODEL=hellord/mxbai-embed-large-v1:f16
    - ENABLE_CONTROL_LAYER=true
    - SKIP_CONTROL_ON_LOW_RISK=true
    - ENABLE_VALIDATION=true
    - VALIDATION_HARD_FAIL=true
    - LOG_LEVEL=INFO
    volumes:
    - ./config:/app/config:ro
    - ./personas:/app/personas
    - ./core:/app/core:ro
    - ./intelligence_modules:/app/intelligence_modules:ro
    - ./mcp_registry.py:/app/mcp_registry.py:ro
    - ./config.py:/app/config.py:ro
    - ./mcp:/app/mcp:ro
    - ./utils:/app/utils:ro
    - ./maintenance:/app/maintenance:ro
    networks:
    - big-bear-lobe-chat_default
    restart: unless-stopped
    depends_on:
    - mcp-sql-memory
    - cim-server
    - sequential-thinking

  jarvis-webui:
    build:
      context: adapters/Jarvis
      dockerfile: Dockerfile
    container_name: jarvis-webui
    ports:
    - 8400:80
    volumes:
      # Live-reload: Mount source files directly into nginx html directory
      - ./adapters/Jarvis/index.html:/usr/share/nginx/html/index.html:ro
      - ./adapters/Jarvis/style.css:/usr/share/nginx/html/style.css:ro
      - ./adapters/Jarvis/js:/usr/share/nginx/html/js:ro
      - ./adapters/Jarvis/static:/usr/share/nginx/html/static:ro
    networks:
    - big-bear-lobe-chat_default
    restart: unless-stopped
    depends_on:
    - jarvis-admin-api

  jarvis-admin-api:
    build:
      context: .
      dockerfile: adapters/admin-api/Dockerfile
    container_name: jarvis-admin-api
    ports:
    - 8200:8200
    environment:
    - MCP_BASE=http://mcp-sql-memory:8081/mcp
    - CIM_URL=http://cim-server:8086
    - SEQUENTIAL_THINKING_URL=http://sequential-thinking:8085
    - DOCUMENT_PROCESSOR_URL=http://document-processor:8087
    - LOG_LEVEL=INFO
    volumes:
    - ./personas:/app/personas
    - ./core:/app/core:ro
    - ./intelligence_modules:/app/intelligence_modules:ro
    - ./custom_mcps:/app/custom_mcps
    - ./mcp_registry.py:/app/mcp_registry.py:ro
    - ./config.py:/app/config.py:ro
    - ./mcp:/app/mcp:ro
    - ./utils:/app/utils:ro
    - ./maintenance:/app/maintenance:ro
    - ./adapters/admin-api/main.py:/app/main.py:ro
    - ./adapters/admin-api/settings_routes.py:/app/settings_routes.py:ro
    networks:
    - big-bear-lobe-chat_default
    restart: unless-stopped
    depends_on:
    - mcp-sql-memory
    - cim-server
    - sequential-thinking
    - document-processor

  mcp-sql-memory:
    build:
      context: ./sql-memory
    container_name: mcp-sql-memory
    environment:
    - DB_PATH=/app/data/memory.db
    - OLLAMA_URL=http://ollama:11434
    - EMBEDDING_MODEL=hellord/mxbai-embed-large-v1:f16
    volumes:
    - ./sql-memory/data:/app/data
    ports:
    - 8082:8081
    networks:
    - big-bear-lobe-chat_default
    restart: unless-stopped

  cim-server:
    build:
      context: ./mcp-servers/cim-server
    container_name: cim-server
    environment:
    - CIM_ROOT=/app/intelligence_modules
    - OLLAMA_BASE=http://ollama:11434
    - LOG_LEVEL=INFO
    volumes:
    - ./intelligence_modules:/app/intelligence_modules
    ports:
    - 8086:8086
    networks:
    - big-bear-lobe-chat_default
    restart: unless-stopped

  sequential-thinking:
    build:
      context: ./mcp-servers/sequential-thinking
    container_name: sequential-thinking
    environment:
    - MCP_SERVER_MODE=true
    - CIM_URL=http://cim-server:8086
    - OLLAMA_BASE=http://ollama:11434
    - LOG_LEVEL=INFO
    ports:
    - 8085:8085
    networks:
    - big-bear-lobe-chat_default
    restart: unless-stopped
    depends_on:
    - cim-server

  document-processor:
    build:
      context: ./mcp-servers/document-processor
    container_name: document-processor
    environment:
    - WORKSPACE_ROOT=/tmp/trion/jarvis/workspace
    - LOG_LEVEL=INFO
    volumes:
    - /tmp/trion/jarvis/workspace:/tmp/trion/jarvis/workspace
    ports:
    - 8087:8087
    networks:
    - big-bear-lobe-chat_default
    restart: unless-stopped

  validator-service:
    build:
      context: ./validator-service/validator-service
    container_name: validator-service
    environment:
    - OLLAMA_BASE_URL=http://ollama:11434
    - EMBEDDING_MODEL=hellord/mxbai-embed-large-v1:f16
    - VALIDATOR_MODEL=qwen2.5:0.5b-instruct
    ports:
    - 8300:8000
    networks:
    - big-bear-lobe-chat_default
    restart: unless-stopped

volumes: {}

networks:
  default:
    name: big-bear-lobe-chat_default
    external: true
  big-bear-lobe-chat_default:
    external: true
