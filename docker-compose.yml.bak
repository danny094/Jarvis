# docker-compose.yml (Mit Sequential Thinking + CIM + Document Processor MCP Services)
#
# Services:
# - lobechat-adapter:     Port 8100 → LobeChat trägt diese URL ein
# - jarvis-admin-api:     Port 8200 → Admin API + Chat für JarvisWebUI
# - jarvis-webui:         Port 8400 → Frontend
# - mcp-sql-memory:       Port 8082 → Memory MCP Server
# - cim-server:           Port 8086 → Frank's CIM MCP Server
# - sequential-thinking:  Port 8085 → Sequential Thinking MCP Server
# - document-processor:   Port 8087 → Document Processor MCP Server (NEU!)
# - validator-service:    Port 8300 → Validator
#
# Netzwerk: big-bear-lobe-chat_default (extern, muss existieren)
#
# SHARED VOLUMES: Core-Code wird live gemountet (kein Rebuild nötig!)

services:
  # ============================================================
  # LOBECHAT ADAPTER
  # ============================================================
  lobechat-adapter:
    build:
      context: .
      dockerfile: adapters/lobechat/Dockerfile
    container_name: lobechat-adapter
    ports:
      - "8100:8100"
    environment:
      # Service URLs
      - OLLAMA_BASE=http://ollama:11434
      - MCP_BASE=http://mcp-sql-memory:8081/mcp
      - VALIDATOR_URL=http://validator-service:8000

      # Model Konfiguration (änderbar!)
      - THINKING_MODEL=deepseek-r1:8b
      - CONTROL_MODEL=qwen3:4b
      - EMBEDDING_MODEL=hellord/mxbai-embed-large-v1:f16

      # Layer Toggles (Speed-Optimierung!)
      - ENABLE_CONTROL_LAYER=true
      - SKIP_CONTROL_ON_LOW_RISK=true

      # Validation
      - ENABLE_VALIDATION=true
      - VALIDATION_HARD_FAIL=true
      - LOG_LEVEL=INFO
    volumes:
      - ./config:/app/config:ro
      - ./personas:/app/personas
      # SHARED VOLUMES - Core Code live gemountet!
      - ./core:/app/core:ro
      - ./intelligence_modules:/app/intelligence_modules:ro
      - ./mcp_registry.py:/app/mcp_registry.py:ro
      - ./config.py:/app/config.py:ro
      - ./mcp:/app/mcp:ro
      - ./utils:/app/utils:ro
      - ./maintenance:/app/maintenance:ro
    networks:
      - big-bear-lobe-chat_default
    restart: unless-stopped
    depends_on:
      - mcp-sql-memory
      - cim-server
      - sequential-thinking

  # ============================================================
  # JARVIS WEB UI (Debug/Chat Interface)
  # ============================================================
  jarvis-webui:
    build:
      context: adapters/Jarvis
      dockerfile: Dockerfile
    container_name: jarvis-webui
    ports:
      - "8400:80"
    networks:
      - big-bear-lobe-chat_default
    restart: unless-stopped
    depends_on:
      - jarvis-admin-api

  # ============================================================
  # JARVIS ADMIN API (Management API for WebUI)
  # ============================================================
  jarvis-admin-api:
    build:
      context: .
      dockerfile: adapters/admin-api/Dockerfile
    container_name: jarvis-admin-api
    ports:
      - "8200:8200"
    environment:
      - MCP_BASE=http://mcp-sql-memory:8081/mcp
      - CIM_URL=http://cim-server:8086
      - SEQUENTIAL_THINKING_URL=http://sequential-thinking:8085
      - DOCUMENT_PROCESSOR_URL=http://document-processor:8087
      - LOG_LEVEL=INFO
    volumes:
      - ./personas:/app/personas
      # SHARED VOLUMES - Core Code live gemountet!
      - ./core:/app/core:ro
      - ./intelligence_modules:/app/intelligence_modules:ro
      - ./custom_mcps:/app/custom_mcps # MCP App Grid - Persistent & Writable
      - ./mcp_registry.py:/app/mcp_registry.py:ro
      - ./config.py:/app/config.py:ro
      - ./mcp:/app/mcp:ro
      - ./utils:/app/utils:ro
      - ./maintenance:/app/maintenance:ro
      - ./adapters/admin-api/main.py:/app/main.py:ro
      - ./adapters/admin-api/settings_routes.py:/app/settings_routes.py:ro
    networks:
      - big-bear-lobe-chat_default
    restart: unless-stopped
    depends_on:
      - mcp-sql-memory
      - cim-server
      - sequential-thinking
      - document-processor

  # ============================================================
  # MCP SQL MEMORY SERVER
  # ============================================================
  mcp-sql-memory:
    build:
      context: ./sql-memory
    container_name: mcp-sql-memory
    environment:
      - DB_PATH=/app/data/memory.db
      - OLLAMA_URL=http://ollama:11434
      - EMBEDDING_MODEL=hellord/mxbai-embed-large-v1:f16
    volumes:
      - ./sql-memory/data:/app/data
    ports:
      - "8082:8081"
    networks:
      - big-bear-lobe-chat_default
    restart: unless-stopped

  # ============================================================
  # CIM SERVER - Frank's Causal Intelligence Module
  # ============================================================
  cim-server:
    build:
      context: ./mcp-servers/cim-server
    container_name: cim-server
    environment:
      - CIM_ROOT=/app/intelligence_modules
      - OLLAMA_BASE=http://ollama:11434
      - LOG_LEVEL=INFO
    volumes:
      # CIM hat exklusiven Zugriff auf intelligence_modules
      - ./intelligence_modules:/app/intelligence_modules
    ports:
      - "8086:8086"
    networks:
      - big-bear-lobe-chat_default
    restart: unless-stopped

  # ============================================================
  # SEQUENTIAL THINKING MCP SERVER
  # ============================================================
  sequential-thinking:
    build:
      context: ./mcp-servers/sequential-thinking
    container_name: sequential-thinking
    environment:
      - MCP_SERVER_MODE=true
      - CIM_URL=http://cim-server:8086
      - OLLAMA_BASE=http://ollama:11434
      - LOG_LEVEL=INFO
    ports:
      - "8085:8085"
    networks:
      - big-bear-lobe-chat_default
    restart: unless-stopped
    depends_on:
      - cim-server

  # ============================================================
  # DOCUMENT PROCESSOR MCP SERVER (NEU!)
  # ============================================================
  document-processor:
    build:
      context: ./mcp-servers/document-processor
    container_name: document-processor
    environment:
      - WORKSPACE_ROOT=/tmp/trion/jarvis/workspace
      - LOG_LEVEL=INFO
    volumes:
      # Workspace für Chunk-Sessions (non-persistent)
      - /tmp/trion/jarvis/workspace:/tmp/trion/jarvis/workspace
    ports:
      - "8087:8087"
    networks:
      - big-bear-lobe-chat_default
    restart: unless-stopped

  # ============================================================
  # VALIDATOR SERVICE
  # ============================================================
  validator-service:
    build:
      context: ./validator-service/validator-service
    container_name: validator-service
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - EMBEDDING_MODEL=hellord/mxbai-embed-large-v1:f16
      - VALIDATOR_MODEL=qwen2.5:0.5b-instruct
    ports:
      - "8300:8000"
    networks:
      - big-bear-lobe-chat_default
    restart: unless-stopped

  # ============================================================
  # OLLAMA (Local LLM Engine)
  # Optimized for Apple Silicon (via Docker translation layer)
  # To use GPU acceleration on Mac with Docker, ensure 'Use Rosetta for x86/amd64' is enabled 
  # in Docker settings if using older images, but standard ollama/ollama usually works fine on CPU/Metal in recent versions.
  # Ideally, run native Ollama for max speed, but this containerized version provides isolation.
  # ============================================================
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - big-bear-lobe-chat_default
    restart: unless-stopped
    # Optional: Resource limits for 16GB RAM M4
    # deploy:
    #   resources:
    #     limits:
    #       memory: 12G

volumes:
  ollama_data:
    driver: local

networks:
  big-bear-lobe-chat_default:
    external: true
